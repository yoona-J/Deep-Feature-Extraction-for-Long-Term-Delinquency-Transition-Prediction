{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCa__wzqiyf-",
        "outputId": "244a85ab-8e19-4717-8178-f475c714db3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-tabnet in /usr/local/lib/python3.12/dist-packages (4.1.0)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.1.2)\n",
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.12/dist-packages (2.7.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from pytorch-tabnet) (2.0.2)\n",
            "Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.12/dist-packages (from pytorch-tabnet) (1.6.1)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.12/dist-packages (from pytorch-tabnet) (1.16.3)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.12/dist-packages (from pytorch-tabnet) (2.9.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.12/dist-packages (from pytorch-tabnet) (4.67.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.13.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.6.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (3.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (3.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch_geometric) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.3->pytorch-tabnet) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# pip install pytorch-tabnet lightgbm xgboost torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "ILkn_Rmhjb1E",
        "outputId": "78906f41-74a0-4b5c-c733-1f3be16a256f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Loan_Institution_Past6M\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4092.4947216898768,\n        \"min\": 0.0,\n        \"max\": 11577.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          11577.0,\n          0.8199015288934958,\n          8.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Overdue_Num_Past6M\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4090.893073605057,\n        \"min\": 0.0,\n        \"max\": 11577.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.35000431890818,\n          42.0,\n          1.6147584739350394\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Overdue_Day_Past6M\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4145.644405071971,\n        \"min\": 0.0,\n        \"max\": 11577.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          7.244795715643085,\n          4209.0,\n          66.78026288870268\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Overdue_Day_Max_Past6M\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4068.7782087986748,\n        \"min\": 0.0,\n        \"max\": 11577.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.918545391724972,\n          569.0,\n          7.439819532575821\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CA_Institution_Past6M\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4092.6811899518343,\n        \"min\": 0.0,\n        \"max\": 11577.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.34283493132935994,\n          7.0,\n          0.7176740147836118\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CA_Institution_Max_Past6M\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4092.6885812006994,\n        \"min\": 0.0,\n        \"max\": 11577.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.27520082923037054,\n          7.0,\n          0.6390328677408829\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CA_UseAmount_Past6M\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 70665.80930559535,\n        \"min\": 0.0,\n        \"max\": 206300.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          11577.0,\n          9055.1286170856,\n          11080.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Card_UseAmount_Past6M\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 158090.91104381336,\n        \"min\": 0.0,\n        \"max\": 458200.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          15093.070743715989,\n          11555.0,\n          11577.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4075.076513549912,\n        \"min\": 8.705401170334396,\n        \"max\": 11577.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          57.241340589099075,\n          57.0,\n          11577.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Gender\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4092.6426290186764,\n        \"min\": 0.4729593708089284,\n        \"max\": 11577.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1.3377386196769456,\n          2.0,\n          0.4729593708089284\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4092.918653039567,\n        \"min\": 0.0,\n        \"max\": 11577.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.06590653882698454,\n          3.0,\n          0.28171550179674326\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Long_Overdue\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4093.022316683579,\n        \"min\": 0.0,\n        \"max\": 11577.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0583916385937635,\n          1.0,\n          0.23449265403273098\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-7c7f4b7f-c53c-4ee3-9a3c-2caa49cb56fa\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Loan_Institution_Past6M</th>\n",
              "      <th>Overdue_Num_Past6M</th>\n",
              "      <th>Overdue_Day_Past6M</th>\n",
              "      <th>Overdue_Day_Max_Past6M</th>\n",
              "      <th>CA_Institution_Past6M</th>\n",
              "      <th>CA_Institution_Max_Past6M</th>\n",
              "      <th>CA_UseAmount_Past6M</th>\n",
              "      <th>Card_UseAmount_Past6M</th>\n",
              "      <th>Age</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Target</th>\n",
              "      <th>Long_Overdue</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>11577.000000</td>\n",
              "      <td>11577.000000</td>\n",
              "      <td>11577.000000</td>\n",
              "      <td>11577.000000</td>\n",
              "      <td>11577.000000</td>\n",
              "      <td>11577.000000</td>\n",
              "      <td>11577.000000</td>\n",
              "      <td>11577.000000</td>\n",
              "      <td>11577.000000</td>\n",
              "      <td>11577.000000</td>\n",
              "      <td>11577.000000</td>\n",
              "      <td>11577.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.819902</td>\n",
              "      <td>0.350004</td>\n",
              "      <td>7.244796</td>\n",
              "      <td>0.918545</td>\n",
              "      <td>0.342835</td>\n",
              "      <td>0.275201</td>\n",
              "      <td>9055.128617</td>\n",
              "      <td>15093.070744</td>\n",
              "      <td>57.241341</td>\n",
              "      <td>1.337739</td>\n",
              "      <td>0.065907</td>\n",
              "      <td>0.058392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.935068</td>\n",
              "      <td>1.614758</td>\n",
              "      <td>66.780263</td>\n",
              "      <td>7.439820</td>\n",
              "      <td>0.717674</td>\n",
              "      <td>0.639033</td>\n",
              "      <td>15757.943613</td>\n",
              "      <td>16220.768876</td>\n",
              "      <td>8.705401</td>\n",
              "      <td>0.472959</td>\n",
              "      <td>0.281716</td>\n",
              "      <td>0.234493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6593.000000</td>\n",
              "      <td>51.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2300.000000</td>\n",
              "      <td>11555.000000</td>\n",
              "      <td>57.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>11080.000000</td>\n",
              "      <td>18468.000000</td>\n",
              "      <td>64.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>8.000000</td>\n",
              "      <td>42.000000</td>\n",
              "      <td>4209.000000</td>\n",
              "      <td>569.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>206300.000000</td>\n",
              "      <td>458200.000000</td>\n",
              "      <td>89.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7c7f4b7f-c53c-4ee3-9a3c-2caa49cb56fa')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7c7f4b7f-c53c-4ee3-9a3c-2caa49cb56fa button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7c7f4b7f-c53c-4ee3-9a3c-2caa49cb56fa');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-75345dc7-cc4b-4df9-b264-e143b722f662\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-75345dc7-cc4b-4df9-b264-e143b722f662')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-75345dc7-cc4b-4df9-b264-e143b722f662 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "       Loan_Institution_Past6M  Overdue_Num_Past6M  Overdue_Day_Past6M  \\\n",
              "count             11577.000000        11577.000000        11577.000000   \n",
              "mean                  0.819902            0.350004            7.244796   \n",
              "std                   0.935068            1.614758           66.780263   \n",
              "min                   0.000000            0.000000            0.000000   \n",
              "25%                   0.000000            0.000000            0.000000   \n",
              "50%                   1.000000            0.000000            0.000000   \n",
              "75%                   1.000000            0.000000            0.000000   \n",
              "max                   8.000000           42.000000         4209.000000   \n",
              "\n",
              "       Overdue_Day_Max_Past6M  CA_Institution_Past6M  \\\n",
              "count            11577.000000           11577.000000   \n",
              "mean                 0.918545               0.342835   \n",
              "std                  7.439820               0.717674   \n",
              "min                  0.000000               0.000000   \n",
              "25%                  0.000000               0.000000   \n",
              "50%                  0.000000               0.000000   \n",
              "75%                  0.000000               0.000000   \n",
              "max                569.000000               7.000000   \n",
              "\n",
              "       CA_Institution_Max_Past6M  CA_UseAmount_Past6M  Card_UseAmount_Past6M  \\\n",
              "count               11577.000000         11577.000000           11577.000000   \n",
              "mean                    0.275201          9055.128617           15093.070744   \n",
              "std                     0.639033         15757.943613           16220.768876   \n",
              "min                     0.000000             0.000000               0.000000   \n",
              "25%                     0.000000             0.000000            6593.000000   \n",
              "50%                     0.000000          2300.000000           11555.000000   \n",
              "75%                     0.000000         11080.000000           18468.000000   \n",
              "max                     7.000000        206300.000000          458200.000000   \n",
              "\n",
              "                Age        Gender        Target  Long_Overdue  \n",
              "count  11577.000000  11577.000000  11577.000000  11577.000000  \n",
              "mean      57.241341      1.337739      0.065907      0.058392  \n",
              "std        8.705401      0.472959      0.281716      0.234493  \n",
              "min       31.000000      1.000000      0.000000      0.000000  \n",
              "25%       51.000000      1.000000      0.000000      0.000000  \n",
              "50%       57.000000      1.000000      0.000000      0.000000  \n",
              "75%       64.000000      2.000000      0.000000      0.000000  \n",
              "max       89.000000      2.000000      3.000000      1.000000  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('Is_longterm.csv')\n",
        "\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nTjknztjtfz",
        "outputId": "2129e99a-cac7-474d-be32-1e5325d8cbae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(9261, 10) (2316, 10)\n",
            "0.05841701760069107 0.05829015544041451\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df.drop(columns=['Target'], inplace=True)\n",
        "\n",
        "X = df.drop(columns=[\"Long_Overdue\"])\n",
        "y = df[\"Long_Overdue\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(X_train.shape, X_test.shape)\n",
        "print(y_train.mean(), y_test.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV0lAOJIkMba"
      },
      "source": [
        "### 10 Baseline Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1daYScrVkGoI",
        "outputId": "dfd30091-f929-4b87-a3f9-ef46bce62dc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train label dist: [8720  541]\n",
            "input_dim: 10\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    import os\n",
        "    import random\n",
        "    import numpy as np\n",
        "    import torch\n",
        "\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    # cudnn deterministic\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(42)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "X = df.drop(columns=[\"Long_Overdue\"])\n",
        "y = df[\"Long_Overdue\"]\n",
        "\n",
        "X_train_np, X_val_np, y_train_np, y_val_np = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_np = scaler.fit_transform(X_train_np.values)\n",
        "X_val_np   = scaler.transform(X_val_np.values)\n",
        "\n",
        "# 5) numpy â†’ tensor\n",
        "X_train = torch.tensor(X_train_np, dtype=torch.float32).to(device)\n",
        "X_val   = torch.tensor(X_val_np,   dtype=torch.float32).to(device)\n",
        "y_train = torch.tensor(y_train_np.values, dtype=torch.float32).to(device)\n",
        "y_val   = torch.tensor(y_val_np.values,   dtype=torch.float32).to(device)\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "\n",
        "print(\"train label dist:\", np.bincount(y_train_np))\n",
        "print(\"input_dim:\", input_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAcxqQMxkWk7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden=64):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze(-1)\n",
        "\n",
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden=64):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.RNN(input_dim, hidden, batch_first=True)\n",
        "        self.fc  = nn.Linear(hidden, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)        # (B, 1, F)\n",
        "        _, h = self.rnn(x)        # h: (1, B, H)\n",
        "        return self.fc(h.squeeze(0)).squeeze(-1)\n",
        "\n",
        "class SimpleLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden=64):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden, batch_first=True)\n",
        "        self.fc   = nn.Linear(hidden, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        _, (h, _) = self.lstm(x)\n",
        "        return self.fc(h.squeeze(0)).squeeze(-1)\n",
        "\n",
        "class CNN1D(nn.Module):\n",
        "    def __init__(self, input_dim, hidden=64):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv1d(1, hidden, kernel_size=3, padding=1)\n",
        "        self.fc   = nn.Linear(hidden * input_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)                            # (B, 1, F)\n",
        "        h = F.relu(self.conv(x))                      # (B, C, F)\n",
        "        h = h.flatten(1)                              # (B, C*F)\n",
        "        return self.fc(h).squeeze(-1)\n",
        "\n",
        "class TinyTransformer(nn.Module):\n",
        "    def __init__(self, input_dim, d_model=64, n_heads=4):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(input_dim, d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_heads,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
        "        self.fc = nn.Linear(d_model, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x).unsqueeze(1)           # (B, 1, D)\n",
        "        h = self.encoder(x).squeeze(1)          # (B, D)\n",
        "        return self.fc(h).squeeze(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRwxR7vpjywy",
        "outputId": "74cd1286-98a9-43f2-fb71-289399440fe8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Train MLP ===\n",
            "=== Train RNN ===\n",
            "=== Train LSTM ===\n",
            "=== Train CNN1D ===\n",
            "=== Train Transformer ===\n",
            "\n",
            "=== Summary (All Models) ===\n",
            "MLP          | ACC=0.8135 | P=0.1707 | R=0.5704 | F1=0.2628 | ROC-AUC=0.7825 | PR-AUC=0.4098\n",
            "RNN          | ACC=0.8407 | P=0.2000 | R=0.5778 | F1=0.2971 | ROC-AUC=0.7879 | PR-AUC=0.4294\n",
            "LSTM         | ACC=0.8169 | P=0.1782 | R=0.5926 | F1=0.2740 | ROC-AUC=0.7918 | PR-AUC=0.4342\n",
            "CNN1D        | ACC=0.8597 | P=0.2189 | R=0.5481 | F1=0.3129 | ROC-AUC=0.7942 | PR-AUC=0.4323\n",
            "Transformer  | ACC=0.8031 | P=0.1691 | R=0.6074 | F1=0.2645 | ROC-AUC=0.7905 | PR-AUC=0.4255\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, average_precision_score\n",
        ")\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "def train_simple(model, X_tr, y_tr, X_va, y_va, epochs=50, lr=1e-3, batch_size=256, seed=42):\n",
        "    model = model.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        n_pos = y_tr.sum().item()\n",
        "        n_neg = len(y_tr) - n_pos\n",
        "        pos_weight = torch.tensor(n_neg / max(1.0, n_pos), device=device, dtype=torch.float32)\n",
        "    crit = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    ds_tr = TensorDataset(X_tr, y_tr)\n",
        "    dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        model.train()\n",
        "        for xb, yb in dl_tr:\n",
        "            opt.zero_grad()\n",
        "            logits = model(xb)\n",
        "            loss = crit(logits, yb)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(X_va)\n",
        "        probs  = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "        preds  = (probs >= 0.5).astype(int)\n",
        "        y_true = y_va.detach().cpu().numpy()\n",
        "\n",
        "    acc  = accuracy_score(y_true, preds)\n",
        "    prec = precision_score(y_true, preds, zero_division=0)\n",
        "    rec  = recall_score(y_true, preds, zero_division=0)\n",
        "    f1   = f1_score(y_true, preds, zero_division=0)\n",
        "\n",
        "    try:\n",
        "        auc = roc_auc_score(y_true, probs)\n",
        "    except ValueError:\n",
        "        auc = np.nan\n",
        "\n",
        "    try:\n",
        "        prauc = average_precision_score(y_true, probs)\n",
        "    except ValueError:\n",
        "        prauc = np.nan\n",
        "\n",
        "    return {\n",
        "        \"ACC\": acc,\n",
        "        \"Precision\": prec,\n",
        "        \"Recall\": rec,\n",
        "        \"F1\": f1,\n",
        "        \"ROC-AUC\": auc,\n",
        "        \"PR-AUC\": prauc,\n",
        "    }\n",
        "\n",
        "\n",
        "results = {}\n",
        "\n",
        "models = {\n",
        "    \"MLP\"        : MLP(input_dim),\n",
        "    \"RNN\"        : SimpleRNN(input_dim),\n",
        "    \"LSTM\"       : SimpleLSTM(input_dim),\n",
        "    \"CNN1D\"      : CNN1D(input_dim),\n",
        "    \"Transformer\": TinyTransformer(input_dim),\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"=== Train {name} ===\")\n",
        "    metrics = train_simple(model, X_train, y_train, X_val, y_val,\n",
        "                           epochs=50, lr=1e-3)\n",
        "    results[name] = metrics\n",
        "\n",
        "print(\"\\n=== Summary (All Models) ===\")\n",
        "for name, m in results.items():\n",
        "    print(\n",
        "        f\"{name:12s} | \"\n",
        "        f\"ACC={m['ACC']:.4f} | \"\n",
        "        f\"P={m['Precision']:.4f} | \"\n",
        "        f\"R={m['Recall']:.4f} | \"\n",
        "        f\"F1={m['F1']:.4f} | \"\n",
        "        f\"ROC-AUC={m['ROC-AUC']:.4f} | \"\n",
        "        f\"PR-AUC={m['PR-AUC']:.4f}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNdXZOmAkeat",
        "outputId": "ccc9c8e1-ee31-4cec-9ca4-e13f360dd3f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stop training because you reached max_epochs = 50 with best_epoch = 48 and best_val_0_auc = 0.7179\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Summary (All Models + TabNet) ===\n",
            "MLP          | ACC=0.8135 | P=0.1707 | R=0.5704 | F1=0.2628 | ROC-AUC=0.7825 | PR-AUC=0.4098\n",
            "RNN          | ACC=0.8407 | P=0.2000 | R=0.5778 | F1=0.2971 | ROC-AUC=0.7879 | PR-AUC=0.4294\n",
            "LSTM         | ACC=0.8169 | P=0.1782 | R=0.5926 | F1=0.2740 | ROC-AUC=0.7918 | PR-AUC=0.4342\n",
            "CNN1D        | ACC=0.8597 | P=0.2189 | R=0.5481 | F1=0.3129 | ROC-AUC=0.7942 | PR-AUC=0.4323\n",
            "Transformer  | ACC=0.8031 | P=0.1691 | R=0.6074 | F1=0.2645 | ROC-AUC=0.7905 | PR-AUC=0.4255\n",
            "TabNet       | ACC=0.9452 | P=0.7222 | R=0.0963 | F1=0.1699 | ROC-AUC=0.7179 | PR-AUC=0.3016\n",
            "[LightGBM] [Info] Number of positive: 541, number of negative: 8720\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000380 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 768\n",
            "[LightGBM] [Info] Number of data points in the train set: 9261, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Info] Start training from score 0.000000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Final Summary (All Models) ===\n",
            "MLP          | ACC=0.8135 | P=0.1707 | R=0.5704 | F1=0.2628 | ROC-AUC=0.7825 | PR-AUC=0.4098\n",
            "RNN          | ACC=0.8407 | P=0.2000 | R=0.5778 | F1=0.2971 | ROC-AUC=0.7879 | PR-AUC=0.4294\n",
            "LSTM         | ACC=0.8169 | P=0.1782 | R=0.5926 | F1=0.2740 | ROC-AUC=0.7918 | PR-AUC=0.4342\n",
            "CNN1D        | ACC=0.8597 | P=0.2189 | R=0.5481 | F1=0.3129 | ROC-AUC=0.7942 | PR-AUC=0.4323\n",
            "Transformer  | ACC=0.8031 | P=0.1691 | R=0.6074 | F1=0.2645 | ROC-AUC=0.7905 | PR-AUC=0.4255\n",
            "TabNet       | ACC=0.9452 | P=0.7222 | R=0.0963 | F1=0.1699 | ROC-AUC=0.7179 | PR-AUC=0.3016\n",
            "LogReg       | ACC=0.8290 | P=0.1871 | R=0.5778 | F1=0.2826 | ROC-AUC=0.7742 | PR-AUC=0.3927\n",
            "SVM          | ACC=0.9413 | P=0.4615 | R=0.0444 | F1=0.0811 | ROC-AUC=0.7992 | PR-AUC=0.2717\n",
            "RandomForest | ACC=0.9495 | P=0.7250 | R=0.2148 | F1=0.3314 | ROC-AUC=0.8152 | PR-AUC=0.4099\n",
            "LightGBM     | ACC=0.9162 | P=0.3145 | R=0.3704 | F1=0.3401 | ROC-AUC=0.7591 | PR-AUC=0.3660\n",
            "XGBoost      | ACC=0.9275 | P=0.3878 | R=0.4222 | F1=0.4043 | ROC-AUC=0.7681 | PR-AUC=0.3858\n"
          ]
        }
      ],
      "source": [
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "\n",
        "tabnet = TabNetClassifier(\n",
        "    n_d=8, n_a=8,\n",
        "    n_steps=3,\n",
        "    gamma=1.3,\n",
        "    n_independent=1,\n",
        "    n_shared=1,\n",
        "    lambda_sparse=1e-3,\n",
        "    optimizer_fn=torch.optim.Adam,\n",
        "    optimizer_params=dict(lr=1e-3),\n",
        "    verbose=0,\n",
        ")\n",
        "\n",
        "tabnet.fit(\n",
        "    X_train_np, y_train_np,\n",
        "    eval_set=[(X_val_np, y_val_np)],\n",
        "    max_epochs=50,\n",
        "    patience=10,\n",
        "    batch_size=1024,\n",
        "    virtual_batch_size=128,\n",
        ")\n",
        "\n",
        "probs_tabnet = tabnet.predict_proba(X_val_np)[:, 1]\n",
        "preds_tabnet = (probs_tabnet >= 0.5).astype(int)\n",
        "\n",
        "acc  = accuracy_score(y_val_np, preds_tabnet)\n",
        "f1   = f1_score(y_val_np, preds_tabnet, zero_division=0)\n",
        "auc  = roc_auc_score(y_val_np, probs_tabnet)\n",
        "\n",
        "results[\"TabNet\"] = {\n",
        "    \"ACC\": acc,\n",
        "    \"Precision\": precision_score(y_val_np, preds_tabnet, zero_division=0),\n",
        "    \"Recall\": recall_score(y_val_np, preds_tabnet, zero_division=0),\n",
        "    \"F1\": f1,\n",
        "    \"ROC-AUC\": auc,\n",
        "    \"PR-AUC\": average_precision_score(y_val_np, probs_tabnet),\n",
        "}\n",
        "\n",
        "\n",
        "print(\"\\n=== Summary (All Models + TabNet) ===\")\n",
        "for name, m in results.items():\n",
        "    print(\n",
        "        f\"{name:12s} | \"\n",
        "        f\"ACC={m['ACC']:.4f} | \"\n",
        "        f\"P={m['Precision']:.4f} | \"\n",
        "        f\"R={m['Recall']:.4f} | \"\n",
        "        f\"F1={m['F1']:.4f} | \"\n",
        "        f\"ROC-AUC={m['ROC-AUC']:.4f} | \"\n",
        "        f\"PR-AUC={m['PR-AUC']:.4f}\"\n",
        "    )\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, average_precision_score\n",
        ")\n",
        "\n",
        "def eval_model(clf, X_val, y_val):\n",
        "    probs = clf.predict_proba(X_val)[:, 1]\n",
        "    preds = (probs >= 0.5).astype(int)\n",
        "\n",
        "    acc  = accuracy_score(y_val, preds)\n",
        "    prec = precision_score(y_val, preds, zero_division=0)\n",
        "    rec  = recall_score(y_val, preds, zero_division=0)\n",
        "    f1   = f1_score(y_val, preds, zero_division=0)\n",
        "    auc  = roc_auc_score(y_val, probs)\n",
        "    pr   = average_precision_score(y_val, probs)\n",
        "\n",
        "    return {\n",
        "        \"ACC\": acc,\n",
        "        \"Precision\": prec,\n",
        "        \"Recall\": rec,\n",
        "        \"F1\": f1,\n",
        "        \"ROC-AUC\": auc,\n",
        "        \"PR-AUC\": pr,\n",
        "    }\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logreg = LogisticRegression(\n",
        "    max_iter=500,\n",
        "    class_weight=\"balanced\",\n",
        "    solver=\"liblinear\"\n",
        ")\n",
        "logreg.fit(X_train_np, y_train_np)\n",
        "\n",
        "results[\"LogReg\"] = eval_model(logreg, X_val_np, y_val_np)\n",
        "\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "svm = SVC(\n",
        "    kernel=\"rbf\",\n",
        "    probability=True,\n",
        "    class_weight=\"balanced\"\n",
        ")\n",
        "svm.fit(X_train_np, y_train_np)\n",
        "\n",
        "results[\"SVM\"] = eval_model(svm, X_val_np, y_val_np)\n",
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=None,\n",
        "    class_weight=\"balanced_subsample\",\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf.fit(X_train_np, y_train_np)\n",
        "\n",
        "results[\"RandomForest\"] = eval_model(rf, X_val_np, y_val_np)\n",
        "\n",
        "\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "lgbm = LGBMClassifier(\n",
        "    n_estimators=500,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=-1,\n",
        "    num_leaves=31,\n",
        "    objective=\"binary\",\n",
        "    class_weight=\"balanced\",\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "lgbm.fit(X_train_np, y_train_np)\n",
        "\n",
        "results[\"LightGBM\"] = eval_model(lgbm, X_val_np, y_val_np)\n",
        "\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "neg, pos = np.bincount(y_train_np)\n",
        "scale_pos_weight = neg / max(pos, 1)\n",
        "\n",
        "xgb = XGBClassifier(\n",
        "    n_estimators=500,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=6,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    eval_metric=\"logloss\",\n",
        "    scale_pos_weight=scale_pos_weight,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "xgb.fit(X_train_np, y_train_np)\n",
        "\n",
        "results[\"XGBoost\"] = eval_model(xgb, X_val_np, y_val_np)\n",
        "\n",
        "print(\"\\n=== Final Summary (All Models) ===\")\n",
        "for name, m in results.items():\n",
        "    print(\n",
        "        f\"{name:12s} | \"\n",
        "        f\"ACC={m['ACC']:.4f} | \"\n",
        "        f\"P={m['Precision']:.4f} | \"\n",
        "        f\"R={m['Recall']:.4f} | \"\n",
        "        f\"F1={m['F1']:.4f} | \"\n",
        "        f\"ROC-AUC={m['ROC-AUC']:.4f} | \"\n",
        "        f\"PR-AUC={m['PR-AUC']:.4f}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3GsIX7QldjS"
      },
      "source": [
        "### Study 1. Transformer+Tabnet Encoder + MLP+LightGBM Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EFGlnIrlczO",
        "outputId": "e0bb8158-aa5c-4d0c-beef-2734a6bbe888"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Fold 1/5] Training DL Encoder...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3244964141.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X_tr_f = torch.tensor(get_data(X_train, train_idx), dtype=torch.float32, device=device)\n",
            "/tmp/ipython-input-3244964141.py:144: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y_tr_f = torch.tensor(get_data(y_train, train_idx), dtype=torch.float32, device=device)\n",
            "/tmp/ipython-input-3244964141.py:145: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X_val_f = torch.tensor(get_data(X_train, val_idx), dtype=torch.float32, device=device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Fold 2/5] Training DL Encoder...\n",
            "\n",
            "[Fold 3/5] Training DL Encoder...\n",
            "\n",
            "[Fold 4/5] Training DL Encoder...\n",
            "\n",
            "[Fold 5/5] Training DL Encoder...\n",
            "\n",
            "--- Final Meta-Stacking ---\n",
            "ðŸš€ Final PR-AUC: 0.2978\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/core.py:774: UserWarning: [03:06:52] WARNING: /workspace/src/common/error_msg.cc:62: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
            "Potential solutions:\n",
            "- Use a data structure that matches the device ordinal in the booster.\n",
            "- Set the device for booster before call to inplace_predict.\n",
            "\n",
            "This warning will only be shown once.\n",
            "\n",
            "  return func(**kwargs)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import average_precision_score\n",
        "import gc\n",
        "\n",
        "class DualGroupStackingEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, fixed_feat_idx, embed_dim=64, num_heads=8):\n",
        "        super(DualGroupStackingEncoder, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.register_buffer('fixed_feat_idx', torch.tensor(fixed_feat_idx, dtype=torch.long))\n",
        "        self.projection = nn.Linear(1, embed_dim)\n",
        "        self.group_type_embedding = nn.Embedding(2, embed_dim)\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.group_tokens = nn.Parameter(torch.zeros(1, 2, embed_dim))\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim, nhead=num_heads, batch_first=True, dropout=0.1\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=3)\n",
        "        self.fs_block = nn.Sequential(nn.Linear(embed_dim, 1), nn.Sigmoid())\n",
        "        self.group_fc = nn.Linear(2 * embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, num_feats = x.shape\n",
        "        device = x.device\n",
        "        x_embedded = self.projection(x.unsqueeze(-1))\n",
        "        group_indices = torch.ones(num_feats, dtype=torch.long, device=device)\n",
        "        group_indices[self.fixed_feat_idx] = 0 \n",
        "\n",
        "        group_info = self.group_type_embedding(group_indices).unsqueeze(0)\n",
        "        x_embedded = x_embedded + group_info\n",
        "\n",
        "        cls_t = self.cls_token.expand(batch_size, -1, -1)\n",
        "        grp_t = self.group_tokens.expand(batch_size, -1, -1)\n",
        "        tokens = torch.cat([cls_t, grp_t, x_embedded], dim=1)\n",
        "\n",
        "        attended = self.transformer(tokens)\n",
        "\n",
        "        z_cls = attended[:, 0, :]\n",
        "        z_group = torch.relu(self.group_fc(attended[:, 1:3, :].reshape(batch_size, -1)))\n",
        "\n",
        "        feat_tokens = attended[:, 3:, :]\n",
        "        importances = self.fs_block(feat_tokens)\n",
        "        masked_feat = (feat_tokens * importances).mean(dim=1)\n",
        "\n",
        "        attn_stats = torch.cat([attended.mean(dim=1), attended.std(dim=1)], dim=1)\n",
        "\n",
        "        return torch.cat([z_cls, z_group, masked_feat, attn_stats], dim=1)\n",
        "\n",
        "def run_full_stacking_pipeline(X_train, y_train, X_test, y_test, n_folds=5):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    fixed_feat_idx = [8, 9]\n",
        "\n",
        "    train_meta_features = np.zeros((len(X_train), 320))\n",
        "    test_meta_features_list = []\n",
        "\n",
        "    scaler = torch.amp.GradScaler('cuda')\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
        "        print(f\"\\n[Fold {fold+1}/{n_folds}] Training DL Encoder...\")\n",
        "\n",
        "        def get_data(df, idx):\n",
        "            return df.iloc[idx].values if hasattr(df, 'iloc') else df[idx]\n",
        "\n",
        "        X_tr_f = torch.tensor(get_data(X_train, train_idx), dtype=torch.float32, device=device)\n",
        "        y_tr_f = torch.tensor(get_data(y_train, train_idx), dtype=torch.float32, device=device)\n",
        "        X_val_f = torch.tensor(get_data(X_train, val_idx), dtype=torch.float32, device=device)\n",
        "\n",
        "        train_loader = DataLoader(TensorDataset(X_tr_f, y_tr_f), batch_size=256, shuffle=True)\n",
        "        encoder = DualGroupStackingEncoder(X_train.shape[1], fixed_feat_idx).to(device)\n",
        "        decoder_mlp = nn.Sequential(nn.Linear(320, 1)).to(device)\n",
        "        optimizer = optim.Adam(list(encoder.parameters()) + list(decoder_mlp.parameters()), lr=0.001)\n",
        "\n",
        "        criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([15.0]).to(device))\n",
        "\n",
        "        encoder.train(); decoder_mlp.train()\n",
        "        for epoch in range(50):\n",
        "            for b_x, b_y in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                with torch.amp.autocast('cuda'):\n",
        "                    feats = encoder(b_x)\n",
        "                    loss = criterion(decoder_mlp(feats).squeeze(), b_y)\n",
        "\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "\n",
        "        encoder.eval()\n",
        "        with torch.no_grad():\n",
        "            train_meta_features[val_idx] = encoder(X_val_f).cpu().numpy()\n",
        "            X_test_tensor = torch.FloatTensor(X_test.values if hasattr(X_test, 'values') else X_test).to(device)\n",
        "            test_meta_features_list.append(encoder(X_test_tensor).cpu().numpy())\n",
        "            del X_test_tensor\n",
        "\n",
        "        del encoder, decoder_mlp, X_tr_f, y_tr_f, X_val_f\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    print(\"\\n--- Final Meta-Stacking ---\")\n",
        "\n",
        "    def ensure_2d(data):\n",
        "        if torch.is_tensor(data):\n",
        "            arr = data.detach().cpu().numpy()\n",
        "        elif hasattr(data, 'to_numpy'):\n",
        "            arr = data.to_numpy()\n",
        "        else:\n",
        "            arr = np.array(data)\n",
        "\n",
        "        if arr.ndim == 1:\n",
        "            arr = arr.reshape(-1, 1)\n",
        "        return arr\n",
        "\n",
        "    X_tr_arr = ensure_2d(X_train)\n",
        "    X_te_arr = ensure_2d(X_test)\n",
        "    y_tr_arr = ensure_2d(y_train).ravel()\n",
        "\n",
        "    test_meta_avg = np.mean(test_meta_features_list, axis=0)\n",
        "\n",
        "    X_train_stack = np.hstack([X_tr_arr, train_meta_features])\n",
        "    X_test_stack = np.hstack([X_te_arr, test_meta_avg])\n",
        "\n",
        "    from xgboost import XGBClassifier\n",
        "    xgb = XGBClassifier(\n",
        "        objective=\"binary:logistic\",\n",
        "        eval_metric=\"aucpr\",\n",
        "        learning_rate=0.05,\n",
        "        max_depth=6,\n",
        "        scale_pos_weight=5.0,\n",
        "        random_state=42,\n",
        "        tree_method=\"hist\",\n",
        "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    )\n",
        "\n",
        "    xgb.fit(X_train_stack, y_tr_arr)\n",
        "    y_prob = xgb.predict_proba(X_test_stack)[:, 1]\n",
        "\n",
        "    print(f\"Final PR-AUC: {average_precision_score(y_test, y_prob):.4f}\")\n",
        "    return y_prob, train_meta_features, test_meta_features_list\n",
        "\n",
        "y_prob_final, oof_train, oof_test = run_full_stacking_pipeline(X_train, y_train, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 832
        },
        "id": "Y7oSrNBLl4jo",
        "outputId": "f591ba0e-a7b8-4dea-a19c-11ac2af4f2ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Study 1 ì¢…í•© ì„±ëŠ¥ í‰ê°€]\n",
            " Accuracy  Precision   Recall  F1-Score  ROC-AUC   PR-AUC\n",
            " 0.945596   0.544554 0.407407  0.466102 0.687598 0.297773\n",
            "\n",
            "[ìµœì  ìž„ê³„ê°’ ê¸°ì¤€ ìƒì„¸ ë¦¬í¬íŠ¸]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.98      0.97      2181\n",
            "           1       0.54      0.41      0.47       135\n",
            "\n",
            "    accuracy                           0.95      2316\n",
            "   macro avg       0.75      0.69      0.72      2316\n",
            "weighted avg       0.94      0.95      0.94      2316\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def evaluate_performance(y_true, y_prob, model_name=\"EasyEnsemble\"):\n",
        "    from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                                 f1_score, roc_auc_score, average_precision_score,\n",
        "                                 classification_report)\n",
        "\n",
        "    y_pred = (y_prob >= 0.5).astype(int)\n",
        "\n",
        "    metrics = {\n",
        "        'Accuracy': [accuracy_score(y_true, y_pred)],\n",
        "        'Precision': [precision_score(y_true, y_pred)],\n",
        "        'Recall': [recall_score(y_true, y_pred)],\n",
        "        'F1-Score': [f1_score(y_true, y_pred)],\n",
        "        'ROC-AUC': [roc_auc_score(y_true, y_prob)],\n",
        "        'PR-AUC': [average_precision_score(y_true, y_prob)]\n",
        "    }\n",
        "\n",
        "    print(f\"\\n[{model_name} ì¢…í•© ì„±ëŠ¥ í‰ê°€]\")\n",
        "    results_df = pd.DataFrame(metrics)\n",
        "    print(results_df.to_string(index=False))\n",
        "\n",
        "    print(\"\\n[ìµœì  ìž„ê³„ê°’ ê¸°ì¤€ ìƒì„¸ ë¦¬í¬íŠ¸]\")\n",
        "    print(classification_report(y_true, y_pred))\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.show()\n",
        "\n",
        "    return\n",
        "\n",
        "evaluate_performance(y_test, y_prob_final, model_name=\"Study 1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjjJOITcAXF6"
      },
      "source": [
        "### Study 2. Feature Extract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixgIS7rgM8Bg",
        "outputId": "b1467cbb-82c5-4ad1-edab-0895d17f3d04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(9261, 10) (2316, 10)\n",
            "0.05841701760069107 0.05829015544041451\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('Is_longterm.csv')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df.drop(columns=['Target'], inplace=True)\n",
        "\n",
        "X = df.drop(columns=[\"Long_Overdue\"])\n",
        "y = df[\"Long_Overdue\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,      \n",
        "    random_state=42,    \n",
        "    stratify=y       \n",
        ")\n",
        "\n",
        "print(X_train.shape, X_test.shape)\n",
        "print(y_train.mean(), y_test.mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQ3ieilsNQw_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class FT_FeatureExtractor(nn.Module):\n",
        "    def __init__(self, n_num_features=10, d_token=16, n_heads=4, n_layers=2):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([nn.Linear(1, d_token) for _ in range(n_num_features)])\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_token))\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_token, nhead=n_heads, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        tokens = [self.layers[i](x[:, i].unsqueeze(-1)) for i in range(len(self.layers))]\n",
        "        x_tok = torch.stack(tokens, dim=1)\n",
        "        batch_size = x.shape[0]\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
        "        x_combined = torch.cat([cls_tokens, x_tok], dim=1)\n",
        "        z = self.transformer(x_combined)\n",
        "\n",
        "        return z[:, 0, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eEAqF9vAWa9",
        "outputId": "25b72a0f-2f0a-4c1c-bf4a-d261cd2ea36e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 0.34599 |  0:00:00s\n",
            "epoch 1  | loss: 0.2474  |  0:00:00s\n",
            "epoch 2  | loss: 0.22305 |  0:00:01s\n",
            "epoch 3  | loss: 0.21576 |  0:00:01s\n",
            "epoch 4  | loss: 0.20944 |  0:00:02s\n",
            "epoch 5  | loss: 0.20679 |  0:00:02s\n",
            "epoch 6  | loss: 0.19905 |  0:00:02s\n",
            "epoch 7  | loss: 0.20263 |  0:00:03s\n",
            "epoch 8  | loss: 0.198   |  0:00:03s\n",
            "epoch 9  | loss: 0.1927  |  0:00:04s\n",
            "epoch 10 | loss: 0.18967 |  0:00:04s\n",
            "epoch 11 | loss: 0.18758 |  0:00:05s\n",
            "epoch 12 | loss: 0.18622 |  0:00:05s\n",
            "epoch 13 | loss: 0.18527 |  0:00:05s\n",
            "epoch 14 | loss: 0.18253 |  0:00:06s\n",
            "epoch 15 | loss: 0.18532 |  0:00:06s\n",
            "epoch 16 | loss: 0.18092 |  0:00:07s\n",
            "epoch 17 | loss: 0.17793 |  0:00:07s\n",
            "epoch 18 | loss: 0.18146 |  0:00:07s\n",
            "epoch 19 | loss: 0.18478 |  0:00:08s\n",
            "epoch 20 | loss: 0.18609 |  0:00:08s\n",
            "epoch 21 | loss: 0.18361 |  0:00:09s\n",
            "epoch 22 | loss: 0.18165 |  0:00:09s\n",
            "epoch 23 | loss: 0.17996 |  0:00:10s\n",
            "epoch 24 | loss: 0.17643 |  0:00:10s\n",
            "epoch 25 | loss: 0.17673 |  0:00:10s\n",
            "epoch 26 | loss: 0.17738 |  0:00:11s\n",
            "epoch 27 | loss: 0.17656 |  0:00:11s\n",
            "epoch 28 | loss: 0.17867 |  0:00:12s\n",
            "epoch 29 | loss: 0.17726 |  0:00:12s\n",
            "epoch 30 | loss: 0.17793 |  0:00:12s\n",
            "epoch 31 | loss: 0.17412 |  0:00:13s\n",
            "epoch 32 | loss: 0.17526 |  0:00:13s\n",
            "epoch 33 | loss: 0.17485 |  0:00:14s\n",
            "epoch 34 | loss: 0.17637 |  0:00:14s\n",
            "epoch 35 | loss: 0.17663 |  0:00:14s\n",
            "epoch 36 | loss: 0.17585 |  0:00:15s\n",
            "epoch 37 | loss: 0.1757  |  0:00:15s\n",
            "epoch 38 | loss: 0.1746  |  0:00:16s\n",
            "epoch 39 | loss: 0.17607 |  0:00:16s\n",
            "epoch 40 | loss: 0.17237 |  0:00:17s\n",
            "epoch 41 | loss: 0.1721  |  0:00:17s\n",
            "epoch 42 | loss: 0.1701  |  0:00:17s\n",
            "epoch 43 | loss: 0.17191 |  0:00:18s\n",
            "epoch 44 | loss: 0.17045 |  0:00:18s\n",
            "epoch 45 | loss: 0.17179 |  0:00:19s\n",
            "epoch 46 | loss: 0.17096 |  0:00:19s\n",
            "epoch 47 | loss: 0.17096 |  0:00:20s\n",
            "epoch 48 | loss: 0.17037 |  0:00:20s\n",
            "epoch 49 | loss: 0.16934 |  0:00:20s\n",
            "--- Data Shape Report ---\n",
            "1. X_np (ì›ë³¸/ì „ì²˜ë¦¬ ë°ì´í„°): (9261, 10)\n",
            "2. ft_cls (CLS í† í°/ìµœì¢… ìž„ë² ë”©): (9261, 16)\n",
            "3. all_step_embs (ë‹¨ê³„ë³„ ìž„ë² ë”©): (9261, 2)\n",
            "4. all_step_masks (ì–´í…ì…˜/ìŠ¤í… ë§ˆìŠ¤í¬): (9261, 10)\n",
            "5. entropy (ëª¨ë¸ í™•ì‹ ë„ ì ìˆ˜): (9261, 1)\n",
            "ìµœì¢… Mega í”¼ì²˜ì…‹ í˜•íƒœ: (9261, 39)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "\n",
        "tabnet_model = TabNetClassifier()\n",
        "\n",
        "tabnet_model.fit(\n",
        "    X_train.to_numpy(),\n",
        "    y_train.to_numpy(),\n",
        "    max_epochs=50\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def get_mega_features(tabnet_model, ft_model, X_input):\n",
        "    if hasattr(X_input, \"values\"):\n",
        "        X_np = X_input.to_numpy()\n",
        "    else:\n",
        "        X_np = X_input\n",
        "\n",
        "    device = next(ft_model.parameters()).device\n",
        "    X_tensor = torch.tensor(X_np).float().to(device)\n",
        "\n",
        "    tabnet_model.network.eval()\n",
        "    ft_model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        steps_output, steps_mask_values = tabnet_model.network.forward_masks(X_tensor)\n",
        "\n",
        "        def to_numpy_robust(data_list):\n",
        "            res = []\n",
        "            for item in data_list:\n",
        "                if torch.is_tensor(item):\n",
        "                    arr = item.detach().cpu().numpy()\n",
        "                elif isinstance(item, np.ndarray):\n",
        "                    arr = item\n",
        "                else:\n",
        "                    try:\n",
        "                        arr = np.array(item)\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "                if arr.ndim == 1:\n",
        "                    arr = arr.reshape(-1, 1)\n",
        "                elif arr.ndim == 0:\n",
        "                    continue\n",
        "\n",
        "                if arr.shape[0] == X_np.shape[0]:\n",
        "                    res.append(arr)\n",
        "            return res\n",
        "\n",
        "        formatted_steps = to_numpy_robust(steps_output)\n",
        "        formatted_masks = to_numpy_robust(steps_mask_values)\n",
        "\n",
        "        if not formatted_steps:\n",
        "            res_emb, _ = tabnet_model.network(X_tensor)\n",
        "            all_step_embs = res_emb.detach().cpu().numpy()\n",
        "        else:\n",
        "            all_step_embs = np.concatenate(formatted_steps, axis=1)\n",
        "\n",
        "        if not formatted_masks:\n",
        "            tab_mask, _ = tabnet_model.explain(X_np)\n",
        "            all_step_masks = tab_mask\n",
        "            m1 = tab_mask\n",
        "        else:\n",
        "            all_step_masks = np.concatenate(formatted_masks, axis=1)\n",
        "            m1 = formatted_masks[0]\n",
        "\n",
        "        entropy = -np.sum(m1 * np.log(m1 + 1e-10), axis=1).reshape(-1, 1)\n",
        "        ft_cls = ft_model(X_tensor).detach().cpu().numpy()\n",
        "        if ft_cls.ndim == 1:\n",
        "            ft_cls = ft_cls.reshape(-1, 1)\n",
        "\n",
        "    print(\"--- Data Shape Report ---\")\n",
        "    try:\n",
        "        print(f\"1. X_np (ì›ë³¸/ì „ì²˜ë¦¬ ë°ì´í„°): {X_np.shape}\")\n",
        "        print(f\"2. ft_cls (CLS í† í°/ìµœì¢… ìž„ë² ë”©): {ft_cls.shape}\")\n",
        "        print(f\"3. all_step_embs (ë‹¨ê³„ë³„ ìž„ë² ë”©): {all_step_embs.shape}\")\n",
        "        print(f\"4. all_step_masks (ì–´í…ì…˜/ìŠ¤í… ë§ˆìŠ¤í¬): {all_step_masks.shape}\")\n",
        "        print(f\"5. entropy (ëª¨ë¸ í™•ì‹ ë„ ì ìˆ˜): {entropy.shape}\")\n",
        "    except AttributeError:\n",
        "        print(f\"1. X_np: {X_np.shape()}\")\n",
        "        print(f\"2. ft_cls: {ft_cls.shape()}\")\n",
        "        print(f\"3. all_step_embs: {all_step_embs.shape()}\")\n",
        "        print(f\"4. all_step_masks: {all_step_masks.shape()}\")\n",
        "        print(f\"5. entropy: {entropy.shape()}\")\n",
        "\n",
        "    X_final = np.concatenate([\n",
        "        X_np,         \n",
        "        ft_cls,      \n",
        "        all_step_embs,\n",
        "        all_step_masks,\n",
        "        entropy\n",
        "    ], axis=1)\n",
        "\n",
        "    return X_final\n",
        "\n",
        "ft_model = FT_FeatureExtractor(n_num_features=10).to(device)\n",
        "X_final_features = get_mega_features(tabnet_model, ft_model, X_train)\n",
        "print(f\"ìµœì¢… Mega í”¼ì²˜ì…‹ í˜•íƒœ: {X_final_features.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAYrllp7P2Rf",
        "outputId": "22c108b1-cf57-4b4a-be7d-f63e826f952b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ìµœì¢… feature shape: (9261, 43)\n"
          ]
        }
      ],
      "source": [
        "from scipy.spatial.distance import mahalanobis\n",
        "import numpy as np\n",
        "\n",
        "def add_mahalanobis_feature(X_final_38, X_train_raw, y_train_raw, eps=1e-8):\n",
        "\n",
        "    X0 = X_train_raw[y_train_raw == 0]\n",
        "    X1 = X_train_raw[y_train_raw == 1]\n",
        "\n",
        "    mu0 = X0.mean(axis=0)\n",
        "    mu1 = X1.mean(axis=0)\n",
        "\n",
        "    cov0 = np.cov(X0, rowvar=False)\n",
        "    cov1 = np.cov(X1, rowvar=False)\n",
        "\n",
        "    VI0 = np.linalg.pinv(cov0)\n",
        "    VI1 = np.linalg.pinv(cov1)\n",
        "\n",
        "    d0_list, d1_list = [], []\n",
        "\n",
        "    for i in range(len(X_final_38)):\n",
        "        x_raw = X_final_38[i, :10] \n",
        "\n",
        "        d0 = mahalanobis(x_raw, mu0, VI0)\n",
        "        d1 = mahalanobis(x_raw, mu1, VI1)\n",
        "\n",
        "        d0_list.append(d0)\n",
        "        d1_list.append(d1)\n",
        "\n",
        "    d0 = np.array(d0_list).reshape(-1, 1)\n",
        "    d1 = np.array(d1_list).reshape(-1, 1)\n",
        "\n",
        "    ratio = d1 / (d0 + eps)\n",
        "    diff  = d0 - d1\n",
        "\n",
        "    X_out = np.concatenate(\n",
        "        [X_final_38, d0, d1, ratio, diff],\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    return X_out\n",
        "\n",
        "X_final_43 = add_mahalanobis_feature(\n",
        "    X_final_features,\n",
        "    X_train.values if hasattr(X_train, \"values\") else X_train,\n",
        "    y_train.values if hasattr(y_train, \"values\") else y_train\n",
        ")\n",
        "\n",
        "print(\"ìµœì¢… feature shape:\", X_final_43.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRmr2zYWP7E8",
        "outputId": "2430ffb0-7321-4062-9ec4-21658ebcc6c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Data Shape Report ---\n",
            "1. X_np (ì›ë³¸/ì „ì²˜ë¦¬ ë°ì´í„°): (2316, 10)\n",
            "2. ft_cls (CLS í† í°/ìµœì¢… ìž„ë² ë”©): (2316, 16)\n",
            "3. all_step_embs (ë‹¨ê³„ë³„ ìž„ë² ë”©): (2316, 2)\n",
            "4. all_step_masks (ì–´í…ì…˜/ìŠ¤í… ë§ˆìŠ¤í¬): (2316, 10)\n",
            "5. entropy (ëª¨ë¸ í™•ì‹ ë„ ì ìˆ˜): (2316, 1)\n",
            "í•™ìŠµ í”¼ì²˜ í˜•íƒœ: (9261, 43)\n",
            "í…ŒìŠ¤íŠ¸ í”¼ì²˜ í˜•íƒœ: (2316, 43)\n"
          ]
        }
      ],
      "source": [
        "X_test_step1 = get_mega_features(tabnet_model, ft_model, X_test)\n",
        "X_test_43 = add_mahalanobis_feature(X_test_step1, X_train, y_train)\n",
        "\n",
        "print(f\"í•™ìŠµ í”¼ì²˜ í˜•íƒœ: {X_final_43.shape}\") # (N, 39)\n",
        "print(f\"í…ŒìŠ¤íŠ¸ í”¼ì²˜ í˜•íƒœ: {X_test_43.shape}\") # (M, 39)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
